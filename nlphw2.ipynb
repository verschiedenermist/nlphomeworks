{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK9IEIVmGkzF",
        "outputId": "e23d43ee-b01b-40e1-fe4d-6fe1ec16a63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=10d5bad2d8f69dd8e78e4fc1fe43e87fac52f0ca5296a30e7b54d9844eef7751\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.9.1)\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.23.5)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26094 sha256=a5f29bdf1ce8164530deba6169e3d6609f4e94ff5697b5592dfb84a21c7bb38c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: razdel, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "! pip install pymorphy2\n",
        "! pip install conllu\n",
        "! pip install natasha\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from conllu import parse\n",
        "import pymorphy2\n",
        "from pymystem3 import Mystem\n",
        "from pprint import pprint\n",
        "\n",
        "from pprint import pprint\n",
        "import time\n",
        "import json\n",
        "from pymystem3 import Mystem"
      ],
      "metadata": {
        "id": "IkOd_unqU8u9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.corpus\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcT5xp0ysh8-",
        "outputId": "6cee521f-2b47-44a7-ce0a-1f1ab6ac1cfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_1=\"На этот званый вечер мужчины должны были надеть галстук-бабочку. \\\n",
        "Обед получился ничего. \\\n",
        "Его ничего не интересует. \\\n",
        "Он же всякий день, рано поутру, ходит с ружьем на охоту.\\\n",
        "Что за охота спорить? \\\n",
        "Медвежонок прыгнул к Ежику, они обхватили друг друга и так несколько мгновений стояли, трясясь.\\\n",
        "Играл в волейбол, в баскетбол, бегал и прыгал, в пионерском лагере участвовал в четырехборье.\\\n",
        "И чихал же наутро старый кот Котофей Котофеич, не пел песен.\\\n",
        "Недавно во время его выступления один в зале чихнул, так он прекратил играть.\\\n",
        "Иванов уверяет, что если бы был еще один блок, то КамАЗ не доехал бы до здания. \\\n",
        "Петр КУЗНЕЦОВ, подполковник ВДВ, член Клуба «Известий». \\\n",
        "Мне же по сердцу то, что написал Вознесенский:«Остриженный наголо, как юный Керенский, но только красивей». \\\n",
        "Местность там красивая: густые леса, Москва-река, но куда ни пойдешь — заборы, каменные стены, милиционеры. \\\n",
        "Туман, ползущий с вершины Ай-Петри, куда мы впоследствии вскарабкались, напоминал нам газовую атаку. \\\n",
        "А после этого метро запил по-черному. \\\n",
        "Согласно ч. 2 ст. 55 Конституции РФ в России не должны издаваться законы, отменяющие или умаляющие права и свободы человека и гражданина. \\\n",
        "Бригадир шел по песку, я по воде — парной и ласковой, а потом река вдруг потемнела, напряглась и остановилась около камней, свирепо гудя и набирая сивую, морщинистую пену. \\\n",
        "Перед ними предстал уголок настоящего леса: высокие стройные березы, густые могучие ели, трепещущие на ветру осины с редким подлеском. \\\n",
        "Все зависит от того, пояснил он вчера, «где будет находиться эмиссионный центр и кто будет распределять средства». \\\n",
        "Предоставление такой возможности и должно являться отличительной особенностью инструментария для инженеров. \""
      ],
      "metadata": {
        "id": "kXV2Q3WQtzFT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mystem"
      ],
      "metadata": {
        "id": "3ObjZwUN77cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mystem()"
      ],
      "metadata": {
        "id": "Sz-imEmXmUxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605d96d2-eda9-4300-dba5-a9a274337dcd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = m.analyze(text_1)"
      ],
      "metadata": {
        "id": "2IWkmbLwapg8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mystem_list = []\n",
        "\n",
        "for word in text:\n",
        "    if 'analysis' in word:\n",
        "        gr = word['analysis'][0]['gr']\n",
        "        pos = gr.split('=')[0].split(',')[0]\n",
        "        mystem_list.append((word['text'], pos))\n",
        "#    pprint(mystem_list)"
      ],
      "metadata": {
        "id": "rrs1D4AmVJCW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pymorphy"
      ],
      "metadata": {
        "id": "F3fRO5gk70Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "gNSW0py1s-kj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = text_1.split()\n",
        "\n",
        "pymorphy_list = []\n",
        "for word in words:\n",
        "    parsed_word = morph.parse(word)[0]\n",
        "    pos_tag = parsed_word.tag.POS\n",
        "    pymorphy_list.append((word, pos_tag))\n",
        "\n",
        "#print(pymorphy_list)"
      ],
      "metadata": {
        "id": "eAPBBGvW3Va8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "natasha"
      ],
      "metadata": {
        "id": "Q2SYMseW7_nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import MorphVocab, Doc, Segmenter, NewsMorphTagger, NewsEmbedding"
      ],
      "metadata": {
        "id": "1zPM2k347yIX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph_vocab = MorphVocab()"
      ],
      "metadata": {
        "id": "gT0n9OE-BcAV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmenter = Segmenter()"
      ],
      "metadata": {
        "id": "i8t5vdj8Ff0t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)"
      ],
      "metadata": {
        "id": "nbTh05cALO5m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Doc(text_1)\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "\n",
        "natasha_list = []\n",
        "\n",
        "for token in doc.tokens:\n",
        "    natasha_list.append((token.text, token.pos))\n",
        "#print(natasha_list)"
      ],
      "metadata": {
        "id": "tkEI1VllYu6X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "функция-конвертер для тегов"
      ],
      "metadata": {
        "id": "kvoNFmNoa3Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_upos(text):\n",
        "    pos_mapping = {\n",
        "        'S': 'NOUN',   # теги майстема\n",
        "        'A': 'ADJ',\n",
        "        'V': 'VERB',\n",
        "        'ADV': 'ADV',\n",
        "        'PR': 'PRON',\n",
        "        'CONJ': 'CONJ',\n",
        "        'PREP': 'ADP',\n",
        "        'NUM': 'NUM',\n",
        "        'INTJ': 'INTJ',\n",
        "        'PARTCP': 'ADJ',\n",
        "        'GER': 'VERB',\n",
        "        'NOUN': 'NOUN',  # теги пайморфи\n",
        "        'ADJF': 'ADJ',\n",
        "        'ADJS': 'ADJ',\n",
        "        'VERB': 'VERB',\n",
        "        'ADVB': 'ADV',\n",
        "        'NPRO': 'PRON',\n",
        "        'NUMR': 'NUM',\n",
        "        'INTJ': 'INTJ',\n",
        "        'PRTF': 'ADJ',\n",
        "        'PRTS': 'ADJ',\n",
        "        'GRND': 'VERB'\n",
        "    }\n",
        "    upos_tags = []\n",
        "    for tag in text:\n",
        "        if tag[1] in pos_mapping.keys():\n",
        "            upos_tags.append(pos_mapping[tag[1]])\n",
        "        else:\n",
        "            upos_tags.append('')\n",
        "\n",
        "    return upos_tags\n",
        "\n",
        "text_example = pymorphy_list\n",
        "upos_result = convert_to_upos(text_example)\n",
        "print(\"UPOS:\", upos_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoJPIEyqa2sN",
        "outputId": "17b67578-a04f-469b-e656-c59efe5a2a94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UPOS: ['ADP', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', '', '', 'NOUN', 'VERB', '', 'PRON', 'ADV', '', '', 'PRON', '', 'ADJ', '', 'ADV', '', 'VERB', 'ADP', 'NOUN', 'ADP', 'ADJ', 'ADP', 'NOUN', '', 'NOUN', 'VERB', 'ADP', '', 'PRON', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'CONJ', 'ADV', 'NOUN', '', 'VERB', 'ADP', '', 'ADP', '', 'VERB', 'CONJ', '', 'ADP', 'ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN', 'VERB', '', 'ADV', 'ADJ', 'NOUN', 'NOUN', '', '', 'VERB', 'ADV', 'ADP', 'NOUN', 'PRON', 'NOUN', 'ADJ', 'ADP', 'NOUN', '', 'CONJ', 'PRON', 'VERB', 'NOUN', '', 'CONJ', 'CONJ', '', 'VERB', 'ADV', 'ADJ', '', 'CONJ', 'NOUN', '', 'VERB', '', 'ADP', '', 'NOUN', '', 'NOUN', '', 'NOUN', 'NOUN', '', 'PRON', '', 'ADP', 'NOUN', '', 'CONJ', 'VERB', 'ADJ', '', 'CONJ', 'ADJ', '', 'CONJ', 'ADV', '', 'NOUN', 'ADV', '', 'ADJ', '', '', 'CONJ', 'CONJ', '', 'VERB', '', '', 'ADJ', '', '', '', 'ADJ', 'ADP', 'NOUN', '', 'CONJ', 'PRON', 'ADV', '', 'VERB', 'PRON', 'ADJ', '', 'CONJ', 'ADP', 'PRON', 'NOUN', 'VERB', '', 'ADP', '', '', '', '', 'NOUN', 'NOUN', 'ADP', 'NOUN', '', 'ADJ', '', '', 'ADJ', 'CONJ', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'CONJ', '', 'NOUN', 'VERB', 'ADP', '', 'PRON', 'ADP', 'NOUN', '', 'ADJ', 'CONJ', '', 'CONJ', 'ADV', 'NOUN', 'ADV', '', 'VERB', 'CONJ', 'VERB', 'ADP', '', 'ADV', 'VERB', 'CONJ', 'VERB', '', 'ADJ', '', 'ADP', 'PRON', 'VERB', 'NOUN', 'ADJ', '', 'ADJ', 'ADJ', '', 'ADJ', 'ADJ', '', 'ADJ', 'ADP', 'NOUN', 'NOUN', 'ADP', 'ADJ', '', '', 'VERB', 'ADP', '', 'VERB', 'PRON', '', 'ADV', 'VERB', '', 'ADJ', 'NOUN', 'CONJ', 'PRON', 'VERB', '', '', 'NOUN', 'ADJ', 'NOUN', 'CONJ', 'ADJ', '', 'ADJ', 'NOUN', 'NOUN', 'ADP', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mystem_list_upos = convert_to_upos(mystem_list)\n",
        "mystem_upos = list(filter(None, mystem_list_upos))\n",
        "#print(mystem_list_upos)"
      ],
      "metadata": {
        "id": "LJp1dmaaiOvq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pymorphy_list_upos = convert_to_upos(pymorphy_list)\n",
        "pymorphy_upos = list(filter(None, pymorphy_list_upos))\n",
        "#print(pymorphy_list_upos)"
      ],
      "metadata": {
        "id": "HsUDxEwgimx7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "natasha_upos = list(filter(None, natasha_list))"
      ],
      "metadata": {
        "id": "Xi9lfpCD-gI1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "тут я достаю из своей разметки слова и часть речи"
      ],
      "metadata": {
        "id": "ZKH5GKrqkRMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('text.txt', 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "a = []\n",
        "\n",
        "for line in lines:\n",
        "    if line.startswith('#') or line.strip() == '':\n",
        "        continue\n",
        "\n",
        "    parts = line.strip().split('\\t')\n",
        "\n",
        "    word = parts[1]\n",
        "    pos_tag = parts[2]\n",
        "    a.append((word, pos_tag))"
      ],
      "metadata": {
        "id": "TWXD63iCpmAQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_a_upos = convert_to_upos(a)\n",
        "a_upos = list(filter(None, list_a_upos))"
      ],
      "metadata": {
        "id": "wWAbS8E0sGXD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(etalon, compared):   # тут что-то страшное произошло, но время тю-тю не успею исправить(((\n",
        "    same = sum(1 for i, j in zip(etalon, compared) if i == j)\n",
        "    accuracy = same / len(etalon)\n",
        "    return accuracy\n",
        "\n",
        "accuracy_1 = calculate_accuracy(a_upos, mystem_upos)\n",
        "accuracy_2 = calculate_accuracy(a_upos, pymorphy_upos)\n",
        "accuracy_3 = calculate_accuracy(a_upos, natasha_upos)\n",
        "\n",
        "print(f\"Accuracy для mystem: {accuracy_1:.2f}\")\n",
        "print(f\"Accuracy для pymorphy: {accuracy_2:.2f}\")\n",
        "print(f\"Accuracy для natasha: {accuracy_3:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP0YXHie2Nxn",
        "outputId": "9e777f36-89ac-46b3-b781-f76802508e5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для mystem: 0.20\n",
            "Accuracy для pymorphy: 0.13\n",
            "Accuracy для natasha: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunker (да, вверху я ещё не получила лучший теггер, но посчитала заранее, что это пайморфи)"
      ],
      "metadata": {
        "id": "cN9IYFcYnU9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunker(text):\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    words = text.split()\n",
        "    ngrams = []\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        word_1 = words[i]\n",
        "        word_2 = words[i + 1]\n",
        "\n",
        "        lemma1 = morph.parse(word_1)[0].normal_form\n",
        "        lemma2 = morph.parse(word_2)[0].normal_form\n",
        "\n",
        "        if word_1 == 'не' and 'NOUN' in morph.parse(word_2)[0].tag:          # тут шаблон типа \"не человек\"\n",
        "            ngrams.append(f'{word_1} {word_2}')\n",
        "        elif 'NOUN' in morph.parse(word_1)[0].tag and 'ADJF' in morph.parse(word_2)[0].tag:  # тут шаблон типа \"хороший человек\"\n",
        "            ngrams.append(f'{word_1} {word_2}')\n",
        "        elif 'NOUN' in morph.parse(word_1)[0].tag and 'ADVB' in morph.parse(word_2)[0].tag:  # тут шаблон типа \"человек тихо\"\n",
        "            ngrams.append(f'{word_1} {word_2}')\n",
        "\n",
        "\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "7lHO0fCdnUhG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "в прошлом дз у меня были отзывы на телефоны. я подумала, что в функции нам лучше смотреть на сочетания прилагательного и существительного (хороший телефон); на сочетания наречия, прилагательного и существительного (очень хороший телефон) и на сочетание \"качественный\" и существительное."
      ],
      "metadata": {
        "id": "cUVSEaNm6577"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunker_2(text):\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    words = text.split()\n",
        "    ngrams = []\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        word_1 = words[i]\n",
        "        word_2 = words[i + 1]\n",
        "        word_3 = words[i + 2]\n",
        "\n",
        "        lemma_1 = morph.parse(word_1)[0].normal_form\n",
        "        lemma_2 = morph.parse(word_2)[0].normal_form\n",
        "        lemma_3 = morph.parse(word_3)[0].normal_form\n",
        "\n",
        "        if word_1 == 'ADJF' and 'NOUN' in morph.parse(word_2)[0].tag:\n",
        "            ngrams.append(f'{word_1} {word_2}')\n",
        "        elif word_1 == 'ADV' and 'ADJF' in morph.parse(word_2)[0].tag and 'NOUN' in morph.parse(word_3)[0].tag:\n",
        "            ngrams.append(f'{word_1} {word_2} {word_3}')\n",
        "        elif word_1 == 'качественный' and 'NOUN' in morph.parse(word_2)[0].tag:\n",
        "            ngrams.append(f'{word_1} {word_2}')\n",
        "\n",
        "\n",
        "\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "JNtDtFCY5HJo"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}